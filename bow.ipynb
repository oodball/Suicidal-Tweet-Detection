{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/geenalimfat/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/geenalimfat/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/geenalimfat/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/geenalimfat/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# notebook configurations\n",
    "pd.options.display.max_colwidth = 1000\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1787, 5593)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"suicide.csv\")\n",
    "\n",
    "df[\"Tweet\"].fillna(\"\", inplace=True)\n",
    "\n",
    "text = df[\"Tweet\"]\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# puts the unique text words into an array \n",
    "bow_matrix = vectorizer.fit_transform(text)\n",
    "\n",
    "# print(sorted(vectorizer.vocabulary_))\n",
    "\n",
    "# puts the count of the word in the array for each tweet\n",
    "bow_arrays = bow_matrix.toarray()\n",
    "\n",
    "bow_arrays.shape\n",
    "\n",
    "\n",
    "\n",
    "# print(bow_arrays)\n",
    "\n",
    "# for i, row in enumerate(bow_arrays):\n",
    "#     # Find indices where the value is greater than 0\n",
    "#     indices = [index for index, value in enumerate(row) if value > 0]\n",
    "#     if indices:  # Print only if there are indices greater than 0\n",
    "#         print(f\"Row {i}: Indices with value greater than 0 -> {indices}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.      0.      0.     ...  0.      0.      0.    ]\n",
      " [ 0.      0.      0.     ...  0.      0.      0.0772]\n",
      " [ 0.      0.      0.     ...  0.      0.     -0.3818]\n",
      " ...\n",
      " [ 0.      0.      0.     ...  0.      0.      0.3818]\n",
      " [ 0.      0.      0.     ...  0.      0.      0.5267]\n",
      " [ 0.      0.      0.     ...  0.      0.      0.6697]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1787, 5594)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sentiment = pd.read_csv(\"sentiment_scores.csv\") \n",
    "df_sentiment.head()\n",
    "\n",
    "compound_scores = df_sentiment[\"compound_scores\"].values\n",
    "\n",
    "#check if the dimensions match\n",
    "if len(bow_arrays) == len(compound_scores):\n",
    "    bow_arrays_with_scores = np.column_stack((bow_arrays, compound_scores))\n",
    "    print(bow_arrays_with_scores)\n",
    "else:\n",
    "    print(\"Dimensions do not match. Cannot append compound scores.\")\n",
    "\n",
    "\n",
    "bow_arrays_with_scores.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 ('COGS118B_WI24')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d32b38566dea232296ad015676f5eb17bd884ce0b9315fa59364119c2a8275d7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
