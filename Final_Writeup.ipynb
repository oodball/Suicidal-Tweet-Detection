{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 118B - Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Names\n",
    "\n",
    "- Audrey Liang\n",
    "- Geena Limfat\n",
    "- Nate Mead\n",
    "- Neha Sharma\n",
    "- Daphne Wu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract \n",
    "\n",
    "We seek to determine a model which accurately predicts the likelihood of a social media post being related to suicidal ideation. Catching behaviors that are indicative of suicidal tendencies could be crucial for early prevention and intervention of suicide completion. The dataset used contains a large body of tweets, each with a ground truth label indicating whether or not the post’s contents are related to suicide. We employed Principal Component Analysis (PCA), K-Means, and Gaussian Mixture Models (GMM), with the Elbow Method and Akaike Information Criterion (AIC) guiding us to identify two main clusters within the tweets, suggesting distinctions between those with and without suicidal ideations.Our findings were substantiated by a high F1 Score, evidencing the model’s capability in accurately classifying tweets. The Elbow Method and AIC confirmed the robustness of our clustering approach, whereas the Rand Index and Adjusted Rand Index offered insights into the clustering quality and areas for further study. Despite our effectiveness with K-Means and GMM, DBSCAN proved less suitable for this dataset. Our analysis, corroborated by our various evaluation metrics, demonstrates the model’s proficiency in detecting tweets with potential suicidal ideation, underscoring the complexity and the need for nuanced analysis in this critical area of mental health monitoring."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "The task of identifying tweets that are indicative of suicidal intent through machine learning techniques intersects with several research areas, notably the fields of sentiment analysis, natural language processing (NLP), and mental health information. The use of machine learning for sentiment analysis has been a subject of extensive research, aiming to understand, interpret, and classify emotions expressed in text. This work lays the groundwork for more specialized applications, including our case of detecting suicidal intent in social media posts.\n",
    "\n",
    "Sentiment analysis itself is a well-established field within NLP, focusing on determining the attitude, emotions, and opinions expressed in text data. The approach has been applied across various domains, from customer feedback analysis to political sentiment assessment. Early works in this particular area utilized simple lexicon-based methods and gradually evolved to incorporate sophisticated machine learning algorithms, like support vector machines and Naive Bayes [<sup>1</sup>](#Liunote). These advancements have significantly improved the accuracy and depth of sentiment analysis.\n",
    "\n",
    "When it comes to detecting distress or suicidal ideation in text, researchers have already explored several methods, including keyword detection, machine learning models, and more recently, deep learning approaches. One of the primary difficulties in this particular area is the nuanced and context-dependent nature of language used to express distress or suicidal thoughts, which requires models to not only understand the semantic content of the text, but also its emotional undertones present [<sup>2</sup>](#Coppersmithnote). Studies have shown that machine learning algorithms can be trained to detect undertones of psychological distress with reasonable accuracy by analyzing patterns in text data derived from social media platforms [<sup>3</sup>](#Denote). In the realm of mental health information, leveraging social media data for mental health surveillance and intervention has become an increasingly researched topic. The ubiquity of social media usage provides a vast dataset of user-generated content that can offer insight into the mental health status of individuals in real-time. This has led to the development of predictive models that aim to identify signs of mental health issues, such as depression and anxiety, as well as suicidal ideation, by analyzing social media posts(4).\n",
    "\n",
    "However, there are still ethical considerations to be had. Privacy and consent, along with the technical challenges of ensuring accuracy and reducing false positives, are significant hurdles that research in this area continues to face. As models become more sophisticated and datasets grow larger, there must be ongoing work to improve the sensitivity and specificity of detection algorithms aimed at sensitive groups of the population. Additionally, interdisciplinary collaboration between computer scientists, psychologists, and other mental health professionals is crucial to address the complex ethical issues surrounding the surveillance of individuals for signs of suicidal intent without their content. It is a very sensitive topic, and people deserve some level of privacy, regardless of what an algorithm might predict. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "The problem we are addressing involves identifying tweets that are indicative of suicidal intent using machine learning techniques. This task can be more broadly categorized under sentiment analysis, but with a specific focus on detecting signals of distress or suicidal thoughts. Our overall goal is to leverage natural language processing and unsupervised machine learning algorithms to uncover patterns in the data that correlate with the ground truth labels from our dataset. We want to develop a machine learning model that can identify tweets with suicidal intent based on their text context. \n",
    "\n",
    "Our dataset consists of tweets, each labeled as either indicative of suicidal intent (1.0) or not (0.0). The tweets will have undergone basic data cleaning. Our feature set will be generated primarily through the Bag of Words approach and sentiment analysis via VADER. Bag of Words will transform the tweet text into a vector of word counts, capturing the frequency of each word within the body of text. The VADER tool will be used to analyze the sentiment of each tweet, producing a compound score that reflects the overall sentiment—positive or negative. We will apply Principal Component Analysis to reduce the dimensionality of the feature space, aiming to retain the most informative aspects while reducing computational complexity. We will implement unsupervised machine learning algorithms like KMeans and Gaussian Mixture Models to cluster the tweets based on the reduced feature set. The goal is to see if clusters formed align with the ground truth labels of suicidal intent.\n",
    "\n",
    "This process makes the problem quantifiable, as it involves quantifying the presence of words and sentiment in tweets, which are then expressed numerically. This problem is measurable, as the alignment of clustering results with ground truth labels can be measured through metrics such as elbow method for cluster cohesion and separation, and F1-score to evaluate the accuracy of the classification model. Our problem setup is replicable, as it relies on structured approaches like Bag of Words, VADER, Principal Component Analysis, and clustering algorithms that can be systemically applied to the tweet dataset or similar datasets with known labels in the future. Our approach seeks not just to detect suicidal intent in tweets based on analysis of their textual content, but also to explore and validate the effectiveness of combining natural language processing with unsupervised machine learning for the particular critical application. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "### Dataset Source: Kaggle Suicidal Tweet Detection Dataset \n",
    "- Dataset Size: The dataset is comprised of 1778 tweets, with two columns.\n",
    "- Observation Description: Each observation has a column with a single Tweet, as well as if the Tweet has been already classified as \"Potential Suicide Post\" or \"Not Suicide Post\".\n",
    "- Critical Variables:\n",
    "    - Tweet Text: Contains the actual content of the tweet.\n",
    "    - Suicide Classification: Whether or not it is a suicidal post. \n",
    "\n",
    "### Data Cleaning Process:\n",
    "\n",
    "Before using the bag of words, we preprocessed the the data using these steps: \n",
    "\n",
    "* Removing Empty Rows: Dropping any Null rows present in the dataset.\n",
    "* Punctuation Removal: We removed punctuation from the `Tweet` column`.\n",
    "* Remove words with number: Eliminates numerical information since it isn't relevant for the purposes of our study.\n",
    "* Remove single-character strings: They often do not provide meaningful information and can introduce noise.\n",
    "* Lowercase: For standardization and consistency.\n",
    "* Remove stop words: Stop words are commonly used words such as \"a,\" \"the,\" \"is\". They do not provide meaning to the sentence, so they have been removed to prevent unneccessary columns.\n",
    "* Part-of-Speech (POS): We were able to lemmatize the words and tag them with their parts of speech. In doing so, we prevent words such as `\"jump\"` and `\"jumping\"` and simply tagging it as its original form `\"jump (n)`\". \n",
    "\n",
    "\n",
    "The detailed code for data cleaning can be found in the [data_cleaning.ipynb](./data_cleaning.ipynb) , while the summary of the cleaning process is provided here for readability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposed Solution\n",
    "\n",
    "As previously stated, the primary goal is to identify tweets that may indicate a risk of suicide. This is a complex and sensitive task that combines natural language processing with machine learning techniques to analyze and classify text data. The first stage will involve data preprocessing, cleaning the tweets by removing special characters, URLs, and user mentions, and potentially irrelevant common words like “the”. This can be done through tokenization and lemmatization, and reduces unnecessary noise or conflict created by these aspects of normal post structure. Next, we will implement feature extraction. To do this, we will apply the Bag of Words (BoW) model to convert the text data into numerical features that represent the frequency each word appears in our dataset. We will also use the VADER sentiment analysis tool to obtain compound sentiment scores for each tweet, adding an extra feature that quantifies the sentiment expressed in the tweet. Given the size of the dataset, dimensionality reduction is also critical. We will implement Principal Component Analysis (PCA) to reduce the high-dimensional feature space obtained from BoW down to a lower-dimensional space, reducing the computational cost and noise while still preserving as much variance as we can. We will then apply KMeans clustering to the PCA-reduced data to identify distinct groups based on the content and overall extracted sentiment from the tweets. Given the ground truth data indicating whether a tweet is related to suicide, we will evaluate the clustering results using the supervised metric of F1-score, as well as conducting the elbow method to assess the quality of different size clusters formed by the unsupervised modelling.\n",
    "\n",
    "For implementation, we will be working off of Python, as it is a programming language with libraries like pandas for data manipulation, scikit-learn for machine learning algorithms, NLTK for natural language processing tasks, and matplotlib for visualization. \n",
    "\n",
    "## Data cleaning and preprocessing description:\n",
    "\n",
    "In order to clean, wrangle, and preprocess the data before analyzing all the Tweets, we followed the steps that are typically used before TF-IDF and sentiment analysis. These steps involved dropping missing values, removing punctuation, excluding words with numbers, eliminating single-character strings, converting text to lowercase, and removing stop words to focus on more significant content.\n",
    "To further refine the data, tweets were tokenized, and words were lemmatized based on their part-of-speech tags, ensuring a more uniform representation of words across different forms. Additionally, the categorical labels in the 'Suicide' column were converted to numerical values (0 for 'Not Suicide post' and 1 for 'Potential Suicide post'), facilitating quantitative analysis. \n",
    "\n",
    "For feature analysis, BoW will be implemented using scikit-learn’s ‘CountVectorizer’, and VADER sentiment analysis will be performed using the ‘vaderSentiment’ library. PCA and KMeans will be executed using the appropriate classes from scikit-learn—’PCA’ and ‘KMeans’ — to cluster the data. To evaluate the clusters, we used the Rand Index and Adjusted Rand Index, which were computed using scikit-learn metrics. We will perform cross-validation on the dataset to ensure the model’s stability and robustness, as well as validate the clustering results against the ground truth using the supervised evaluation metric F1-score.\n",
    "\n",
    "With this approach, our goal is to to provide a robust and interpretable solution for identifying tweets indicating suicide risk, using te power of sentiment analysis and clustering techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "\n",
    "Given our context of using machine learning techniques to identify tweets indicative of suicidal intent, it is of course crucial to select evaluation metrics that accurately reflect the model’s ability to detect such sensitive content reliably and accurately. We have chosen the F1-score, Rand Index, and Adjusted Rand Index as viable options, each offering unique insights into our model’s performance.\n",
    "First, F1-score is a balanced measure that considers both precision and recall of a classification model. Precision is the ratio of true positive results to all positive results predicted, while recall measures the ratio of true positive results to all actual positive results. The F1-score is the mean of precision and recall, offering a single metric to assess the balance between both. This is particularly important in our context of detecting suicidal intent, where both false negatives of failing to identify suicidal tweets and false positives of wrongly identifying tweets as suicidal carry significant consequences. \n",
    "\n",
    "- F1 = 2 * (Precision * Recall)/(Precision + Recall)\n",
    "- Precision = (True Positive)/(True Positive + False Positive)\n",
    "- Recall = (True Positive)/(True Positive + False Negative)\n",
    "\t\n",
    "Next, Rand Index is a measure of similarity between two data clusterings. Given a set of elements and two partitions of said elements, Rand Index quantifies how similar the partitions are. It considers all pairs of elements and checks whether pairs are assigned in the same or different clusters in predicted and true clusterings.\n",
    "\n",
    "- Rand Index = (Number of pairs of elements in same set + Number of pairs of elements in different sets)/(total number of possible pairs in dataset)\n",
    "\n",
    "The Adjusted Rand Index adjusts the Rand Index for the random chance grouping of elements. It provides a greater accuracy in the measure of true agreement between the clustering and the ground truth, by normalizing with respect to the expected Random Index under random partitioning. The Adjusted Rand Index has a range of [-1,1], where 1 would indicate perfect agreement, 0 would indicate random agreement, and negative values would indicate agreement less than expected by chance. This is well suited for evaluating clustering models like those used for grouping tweets based on their content or sentiment, which could indirectly contribute to identifying suicidal intent.\n",
    "\n",
    "- **F1 Score**: 0.9311377245508982 - measure of model accuracy considering presicion and recall of the model's predictions. Particularly usefil in binary classifications (which we are doing). K-means produced the highest F1 score compared to GMM and DBSCAN. \n",
    "- **K-Means Rand**: 0.6050382484832498 - computes similarity between two clusters. K-means produced the second highest score of 0.605 this indicates moderate agreement between the k-means clusters and the truth labels. \n",
    "- **K-Means Adjusted Rand**: 0.1793769407924069 - the rand index but corrected-for-chance. As it is relatively low, there may be some but not a strong agreement between clustering labels and truth labels, and some agreement may be due to chance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results:\n",
    "## Analysis of Tweet Data for Identifying Suicidal Tendencies\n",
    "\n",
    "The primary goal of our research is to detect and uncover patterns within tweet data that could suggest a risk of suicide. We do this using unsupervised machine learning techniques. We utilize the textual content of tweets, transform them into numerical data through Bag of Words (BoW) and sentiment analysis using VADER. Following this, we perform dimensionality reduction, clustering, and model evaluation techniques in order to analyze our data effectively.\n",
    "\n",
    "## Data Preprocessing and Feature Extraction\n",
    "We began by loading our dataset, which includes tweet texts and a binary indicator of suicidal tendency (whether or not the post might be suicidal). However, our analysis focused solely on the text and we dropped the column of data that would indicate suicidal tendency. After excluding missing data (dropping empty rows), we processed 1000 features using BoW and appended sentiment scores from VADER to our feature set.\n",
    "\n",
    "## Dimensionality Reduction with PCA\n",
    "Principal Component Analysis (PCA) was used to reduce the dimensionality of our feature set, simplifying the high-dimensional BoW data into two principal components for more ease of visualization and clustering.\n",
    "\n",
    "## Determining Optimal Clusters with Elbow Method\n",
    "We used the Elbow Method to identify the optimal number of clusters for K-Means clustering. Our analysis found a distinct \"elbow\" at \\( k=2 \\), which suggested that two clusters was the best choice for clustering our data (see Figure below).\n",
    "\n",
    "<img src=\"pics/kmeansElbow.png\" alt=\"Elbow Method\" width=\"400\" height=\"300\"/>\n",
    "\n",
    "**Analysis/explaination of graph** This graph plots the Within-Cluster Sum of Squares (WCSS) against the number of clusters. WCSS serves as a measure of the variance within each cluster; a lower WCSS signifies that data points are more tightly grouped around the cluster centroids. The graph reveals that the optimal number of clusters is two, which is evidenced by the plateau in the rate of decrease in WCSS, suggesting that increasing the number of clusters beyond this point yields minimal benefit. This finding is consistent with our dataset, which requires categorizing tweets into two distinct categories: suicidal and non-suicidal.\n",
    "\n",
    "## Clustering with K-Means ([K-Means Clustering Notebook](./kmeans_gmm_db_visualization.ipynb))\n",
    "We applied K-Means with the chosen number of clusters which revealed two distinct groupings in our PCA-reduced feature space, as shown in the figure below. The centroids of the clusters are marked with a red x that indicates the central points around which the tweets are grouped.\n",
    "\n",
    "<img src=\"pics/kmeansCluster.png\" alt=\"K-Means Clustering with PCA\" width=\"400\" height=\"300\"/>\n",
    "\n",
    "**Analysis/explaination of graph**: A scatter plot displays tweets clustered via PCA, with two dimensions representing PCA features. Each tweet, color-coded by cluster, is categorized into potentially suicidal or non-suicidal sentiment. Red crosses mark centroids. \n",
    "\n",
    "#### Evaluation metrics for K-means clustering: \n",
    "- **F1 Score**: 0.9311377245508982 - K-means produced the highest F1 score compared to GMM and DBSCAN. \n",
    "- **K-Means Rand**: 0.6050382484832498 - K-means produced the second highest score of 0.605 this indicates moderate agreement between the k-means clusters and the truth labels. \n",
    "- **K-Means Adjusted Rand**: 0.1793769407924069 - K-means produces a relatively low score. There may be some but not a strong agreement between clustering labels and truth labels, and some agreement may be due to chance.\n",
    " \n",
    "The main objective is to closely reflect the labeled classification of tweets into \"suicidal\" or \"not suicidal\". In this case, the high F1 Score suggests success and indicates that the clustering effectively discerns sentiment, however, the low adjusted rand index cautions an overstated success. \n",
    "\n",
    "## Model Selection with Gaussian Mixture Model (GMM) ([GMM Clustering Notebook](./kmeans_gmm_db_visualization.ipynb))\n",
    "To verify the clustering results and explore alternative models, we fitted a Gaussian Mixture Model (GMM). By using the Akaike Information Criterion (AIC), we identified the best fit model complexity. The AIC suggested a preference for a model with fewer components. This aligned with our K-Means results, as depicted in the figure below. The resulting clusters from the GMM, as visualized, and it also suggested a meaningful segmentation of the data.\n",
    "\n",
    "<img src=\"pics/AICGMM.png\" alt=\"AIC for Model Selection\" width=\"400\" height=\"300\"/>\n",
    "\n",
    "**Analysis/explaination of graph**: To determine the optimal number of components for GMM clustering, we plotted the Akaike Information Criterion (AIC) against GMM components where a lower AIC signifies a better fit with considerations for complexity penalties. There is a noticeable decline from 1 to 3 components, followed by a plateau, indicating minimal improvement beyond three. Consequently, the ideal number of components is two, suggesting that the data is best represented by a mixture of two distributions. This segmentation effectively divides tweets into positive and negative sentiment categories, corresponding to non-suicidal and potentially suicidal sentiments. Visualizations confirm the presence of two distinct groups within the dataset.\n",
    "\n",
    "<img src=\"pics/GMMCluster.png\" alt=\"Gaussian Mixture Model Clustering\" width=\"400\" height=\"300\"/>\n",
    "\n",
    "**Analysis/explaination of graph**: The scatter plot displays GMM clustering results on PCA-reduced tweet data. Each point represents a tweet, positioned by its values on the first two principal components. Clustering reveals two sentiment groups, possibly indicating suicidal ideation. \n",
    "\n",
    "#### Evaluation metrics for GMM: \n",
    "- **GMM F1 Score**: 0.9304973037747154 - produced the second highest score behind k-means, sugessting that the clusters produced by GMM align with the true binary labels in the tweet dataset. \n",
    "- **GMM Rand**: 0.6040120083908003 - produced the lowest score of 0.604. Still, it indicates moderate agreement between the GMM clusters and the ground truth labels.\n",
    "- **GMM Adjusted Rand**: 0.177437612438209 - produced a relatively low score. There may be some but not a strong agreement between clustering labels and ground truth labels, though some agreement may still be due to chance.\n",
    "\n",
    "As mentioned, the main objective is to closely reflect the labeled classification of tweets into \"suicidal\" or \"not suicidal\". Similar to the K-means clustering, GMM clustering produces a high F1 Score suggesting success, however, the low adjusted rand index indicated that there may be an overstated success. \n",
    "\n",
    "## Alternative Clustering with DBSCAN ([DBSCAN Clustering Notebook](./kmeans_gmm_db_visualization.ipynb))\n",
    "We also took a look at using Density-Based Spatial Clustering of Applications with Noise (DBSCAN) as an alternative method to identify clusters based on density. However, the results we got, shown in the figure below, did not yield distinct clusters when compared to K-Means or GMM.\n",
    "\n",
    "<img src=\"pics/DBSCANCluster.png\" alt=\"DBSCAN Clustering with PCA\" width=\"400\" height=\"300\"/>\n",
    "\n",
    "**Analysis/explaination of graph**: This scatter plot shows DBSCAN clustering results on PCA-reduced tweet data. DBSCAN identifies clusters based on density, without presetting cluster numbers. Each point represents a tweet, colored by cluster. Noise points are labeled with -1. Algorithm parameters were optimized for silhouette score. Clusters may indicate varying levels of concern for suicidal ideation. \n",
    "\n",
    "#### Evaluation metrics for DBSCAN: \n",
    "- **DBSCAN F1 Score**: 0.908590645062672 - produced a high score, but the lowest of the three models used. Still, it suggests that the clusters produced by DBSCAN align with the true binary lables in the tweet dataset. \n",
    "- **DBSCAN Rand**: 0.632388740249463 - produced the highest score between the models used of 0.604. It indicates more but still moderate agreement between the DBSCAN clusters and the truth labels.\n",
    "- **DBSCAN Adjusted Rand**: 0.2432863426143979 - produced the highest score of the three models. Compared to the other models, there more of an agreement between clustering labels and ground truth labels, some agreement may still be due to chance.\n",
    "\n",
    "While the quantitative metrics suggest that DBSCAN's performance is on par with, if not slightly better than, K-Means and GMM, the visual comparison of the clusters shows that DBSCAN created clusters that are less cohesive and it labeled many points as outliers. This is not as desierable to the kind of analysis we want to do for our tweet data. Therefore, GMM and K-means is more preferable over DBSCAN.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion\n",
    "\n",
    "### Interpreting the Result\n",
    "Our analysis, through using Principal Component Analysis (PCA), K-Means clustering, and Gaussian Mixture Models (GMM), guided by the Elbow Method and Akaike Information Criterion (AIC) to better ensure best, revals two primary clusters within the tweet dataset. These clusters are likely associated to tweets with and without indications of suicidal ideation, based on sentiment valences derived from the data (through processing 1000 features using BoW and appending sentiment scores from VADER to our feature set), which can be seen in the `sentiment_labels` text files.\n",
    "\n",
    "Though DBSCAN achieved the highest Rand and Adjusted Rand Index scores, it produced clusters with many outliers when visually assessed. This suggests it may not be the best fit for the structured sentiment analysis of tweet data. K-Means showed the strongest alignment with the labeled data according to the F1 Score. This indicates more effective clustering aligned with the binary classification of tweets, despite its lower Adjusted Rand Index, which implyed some of the clustering could be coincidental. GMM's performance was comparable to K-Means but with a slightly lower F1 Score. This suggests that the model is a viable alternative, though it had the lowest Rand Index, indicating moderate alignment with true labels.\n",
    "\n",
    "**Main Point**: \n",
    "The main objective is to closely reflect the labeled classification of tweets into \"suicidal\" or \"not suicidal\". The combination of PCA with K-Means clustering effectively segregated the tweets into two distinct categories. This is further supported by a high F1 Score, indicating that our model is proficient in distinguishing between tweets that may or may not be related to suicide ideation.\n",
    "\n",
    "**Secondary Points**:\n",
    "1. Validation through Elbow Method and AIC: The Elbow Method clearly indicated an optimal number of clusters, further corroborated by the AIC during the application of GMM. This dual-validation approach ensures that our clustering choices are empirically sound and methodologically robust.\n",
    "   \n",
    "2. Algorithm Selection: The comparative analysis reveals that DBSCAN was less effective than K-Means and GMM for this particular dataset. This underscores the importance of selecting appropriate clustering algorithms that align with the data's characteristics and the analysis objectives. In comparison to the Gaussian Mixture Model and K-Means, we were unable to get proper clusters. \n",
    "\n",
    "3. Evaluation Metrics Insight: The F1 Score validates the effectiveness of our clustering approach, while the Rand Index and Adjusted Rand Index provide a nuanced view of the clustering quality. These metrics confirm the validity of our model selections but also highlight the complexities within the tweet data that warrant further investigation. While the F1 Scores may have been very high, we also saw considerably lower Adjusted Rand Indices, which indicate that some part of these models could be in part due to chance clustering. \n",
    "\n",
    "4. Algorithm Exploration: Our exploratory approach, which included K-Means, GMM, and DBSCAN, demonstrates the value of assessing multiple clustering methods. The performance alignment between GMM and K-Means reinforces our findings and supports a comprehensive examination of the data. This is also something we are expecting, as we have the ground truth labels, indicating that the data was only split into two clusters: Potential Suicide Posts and Not Suicide Posts. \n",
    "\n",
    "### Limitations\n",
    "\n",
    "While the results we got are promising, there are still limitations to the approach we used. The range of the data and the clustering methods we used are heavily reliant on the chosen hyperparameters. The exploration of a broader hyperparameter space may enhance model performance. Additionally, the dimensionality reduction through PCA, although may be beneficial for visualization, it could result in the loss of important information that may improve clustering accuracy. Additionally, more data points would have been nicer, as well as a way to clean up the user's twitter handle, since those don't add any value to our analysis, and we would have had a better idea of what constitutes as a potentially suicidal or non suicidal post. \n",
    "\n",
    "### Ethics & Privacy\n",
    "\n",
    "It’s critical when examining large, sensitive groups of users on any platform to avoid singling out any one person because of their online activity. Suicide as a topic is extremely sensitive and personal, and many post to Twitter related to suicidal ideations and depression as a way to vent, rather than with the intention of following through. It’s important to keep the activities and identities of individuals private, and focus on overall trends within populations rather than a specific person’s expressions of personal issues and feelings. Twitter profiles outline and illustrate individual activity that can link to personal emails, and run concurrently with posts irrelevant to our study. We have tried to anonymize profiles as best we can to minimize  identifying information that could out specific users for the content they produce and discuss. Additionally, we would like to avoid unintended consequences such as the misuse of these results or potential stigmatization of people based on this model output. \n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Our analysis indicated that machine learning can aid in providing valuable insights into identifying patterns associated with suicidal tendencies in social media texts. Our study's main takeaway is the potential of machine learning in identifying tweets with suicidal content, with the F1 Score substantiating the effectiveness of our clustering approach. The combination of PCA with K-Means clustering worked most effectively. The work fits within the broader context of mental health monitoring through social media, a growing field that leverages data analytics to offer timely interventions. Future work should look into richer feature sets, alternative dimensionality reduction techniques, and the ethical deployment of such models. Enhanced model tuning and validation, alongside collaboration with mental health professionals, could see these techniques contributing significantly to preventive healthcare measures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Footnotes\n",
    "<a name=\"Liunote\"></a>1.[^](#liu): Liu, B. (2012). Sentiment Analysis and Opinion Mining. Synthesis Lectures on Human Language Technologies.<br> \n",
    "<a name=\"Coppersmithnote\"></a>2.[^](#Coppersmith): Coppersmith, G., Dredze, M., & Harman, C. (2014). Quantifying mental health signals in Twitter. Proceedings of the Workshop on Computational Linguistics and Clinical Psychology.<br>\n",
    "<a name=\"Denote\"></a>3.[^](#De):De Choudhury, M., Gamon, M., Counts, S., & Horvitz, E. (2013). Predicting Depression via Social Media. International AAAI Conference on Web and Social Media.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 ('COGS118B_WI24')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "d32b38566dea232296ad015676f5eb17bd884ce0b9315fa59364119c2a8275d7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
